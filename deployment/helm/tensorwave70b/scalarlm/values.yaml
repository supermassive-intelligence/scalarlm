image:
  repository: sudnya/scalarlm-amd
  tag: v0.90
  pullPolicy: Always

service:
  type: ClusterIP
  api_port: 8000
  vllm_ports: 
    - 8001
    - 8002
  externalIP: 64.139.222.101

replicaCount:
  api: 1
  vllm: 2  # Two vLLM instances as shown in docker-compose

jobs_pvc:
  storageClass: local-path
  size: 100Gi

cache_pvc:
  storageClass: local-path
  size: 200Gi

model: meta-llama/Llama-3.3-70B-Instruct
max_model_length: 8192
gpu_memory_utilization: 0.95

training_gpus: 2
inference_gpus: 6

max_train_time: 86400
